# Knowledge ETL

Extract-Transform-Load pipeline that converts URLs, images, and documents to **pure text Markdown**.

## Core Concept

```
输入                     处理                      输出
┌─────────┐         ┌───────────┐          ┌─────────────┐
│ URL     │         │           │          │             │
│ 图片    │ ──────▶ │ extractor │ ──────▶  │ 纯文本       │
│ PDF     │         │  (Agent)  │          │ Markdown    │
└─────────┘         └───────────┘          └─────────────┘
                          │
                          ▼
                ┌─────────────────┐
                │ 图片 → 文字描述  │
                │ (不保存文件)     │
                └─────────────────┘
```

**关键特性：所有内容（包括图片）都转换为纯文本，不生成文件。**

## 为什么需要这个？

| 问题 | 解决方案 |
|------|----------|
| 读取大图片时 "prompt too large" | Agent 在独立上下文处理，主对话不超限 |
| 网页图文混合难以提取 | 统一转换为纯文本 Markdown |
| 多图片处理容易超限 | 逐张处理，结果汇总为文本 |

---

## 系统架构

### 核心约束

```
┌─────────────────────────────────────────────────────────────────┐
│                     绝对红线                                     │
│                                                                  │
│   ❌ 禁止触发 "Prompt Too Long" 错误                            │
│   ❌ 禁止阻塞用户（遇障碍必须交互）                              │
│   ❌ 禁止丢失关键信息（无声失败）                                │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### MCP 工具隔离性

MCP 工具（如 Playwright）不会自动传播到子代理（subagent）。

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         工具可用性架构                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   主上下文 (Main Context)                                                │
│   ├── 标准工具: Read, Write, Bash, Glob, Grep...                        │
│   ├── MCP 工具: mcp__playwright__*, mcp__deepwiki__*...                │
│   └── 可调用子代理                                                       │
│                                                                          │
│   子代理 (Subagent via Task)                                             │
│   ├── 标准工具: Read, Write, Bash, Glob, Grep...                        │
│   └── ❌ MCP 工具不可用                                                  │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

**解决方案**:
1. **主上下文处理 URL 提取**: 使用 Playwright 获取页面内容并保存到临时文件
2. **子代理处理本地文件**: 读取保存的文件（快照、截图）进行内容处理
3. **分工明确**: 主上下文 = 网络交互，子代理 = 内容处理（隔离上下文防溢出）

### 五层架构

```
┌─────────────────────────────────────────────────────────────────────────┐
│                          Knowledge ETL Pipeline                          │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   Layer 1: 输入解析层 (Input Parser)                                     │
│   ├── URL / 文件 / Glob / 目录 检测                                      │
│   └── 生成任务队列 TaskQueue[]                                           │
│                                                                          │
│   Layer 2: 安全控制层 (Safety Controller)                                │
│   ├── 预检模块: 文件大小/累计大小/文件数量检查                           │
│   └── 策略分配: DIRECT / COMPRESS / CHUNK / SUMMARY / SCREENSHOT        │
│                                                                          │
│   Layer 3: 提取执行层 (Extraction Engine)                                │
│   ├── 障碍检测: 登录拦截 / 反爬 / 权限 / 超时                            │
│   └── 内容提取器: Web / Image / PDF / Text Extractor                    │
│                                                                          │
│   Layer 4: 理解与转换层 (Understanding Engine)                           │
│   ├── 结构化理解: 原始内容 → 中间表示 (IR)                               │
│   └── 图片理解: 流程图 / 架构图 / 截图 / 图表 专用描述                   │
│                                                                          │
│   Layer 5: 输出生成层 (Output Generator)                                 │
│   └── Markdown / JSON / Skill / Plugin 格式输出                          │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 数据流

```
输入                处理                  存储              输出

URL ─────┐
         │      ┌─────────────┐      ┌─────────────┐
文件 ────┼─────→│  任务队列   │─────→│  临时存储   │
         │      │  TaskQueue  │      │  /tmp/etl/  │
目录 ────┘      └──────┬──────┘      └──────┬──────┘
                       │                    │
                       ▼                    │
                ┌─────────────┐             │
                │  执行计划   │             │
                │  ExecPlan   │             │
                └──────┬──────┘             │
                       │                    │
                       ▼                    │
         ┌─────────────────────────┐        │
         │     批次处理循环        │        │
         │                         │        │
         │  ┌─────────────────┐   │        │     ┌─────────────┐
         │  │ Agent Context   │   │────────┼────→│   摘要文件   │
         │  │  (隔离)         │   │        │     │  .summary   │
         │  │ 提取 → 理解     │   │        │     └─────────────┘
         │  └─────────────────┘   │        │
         └─────────────────────────┘        │
                       │                    │
                       ▼                    │
                ┌─────────────┐             │
                │  汇总阶段   │◀────────────┘
                │ 生成最终IR  │
                └──────┬──────┘
                       │
                       ▼
         ┌─────────────────────────────┐
         │        最终输出              │
         │  - formatted.md (Markdown)  │
         │  - structured.json (JSON)   │
         │  - skill.md (Skill)         │
         └─────────────────────────────┘
```

---

## 安装

```bash
claude plugin install knowledge-etl@ai-rd
```

### 前置条件

配置 Playwright MCP：

```json
{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["@playwright/mcp@latest", "--caps=vision,pdf"]
    }
  }
}
```

## 使用

### 命令

```bash
# 提取网页（文本+图片描述）
/knowledge-etl:extract https://example.com/article

# 分析大图片
/knowledge-etl:extract ./screenshot-4k.png

# 批量图片处理
/knowledge-etl:extract "./docs/*.png"

# PDF 提取
/knowledge-etl:extract ./document.pdf
```

### 输出示例

```markdown
---
source: https://example.com/docs
title: 系统架构文档
extracted_at: 2025-12-12T21:30:00+08:00
---

# 系统架构文档

## 概述

本文档介绍系统的整体架构设计...

---
**[图片: 整体架构]** 图片展示了系统的三层架构：
1. 表现层：Web 前端和移动 App
2. 业务层：API Gateway + 微服务集群
3. 数据层：MySQL + Redis + ElasticSearch
各层之间通过 REST API 和消息队列通信。
---

## 详细设计

...后续文本内容...
```

---

## 工作原理

### 独立上下文解决 "prompt too large"

```
主对话                           Extractor Agent
  │                                    │
  │ "提取这个网页"                      │
  │ ─────────────────────────────────▶ │
  │                                    │ ┌─────────────────┐
  │                                    │ │ 导航到 URL       │
  │                                    │ │ 提取文本        │
  │                                    │ │ 读取图片(压缩后) │
  │                                    │ │ 转换为文字描述   │
  │                                    │ └─────────────────┘
  │                                    │
  │ "纯文本 Markdown 结果"             │
  │ ◀───────────────────────────────── │
  │                                    │
主对话永远不会接收图片数据！
```

### URL 提取流程

```
1. 导航到 URL
2. Snapshot 提取文本
3. 检测：需要登录？反爬？
   ├─ 需要登录 → 提示用户
   ├─ 反爬保护 → 截图 fallback
   └─ 正常内容 → 继续
4. 提取所有文本
5. 对每张图片：
   ├─ 压缩（如需要）
   ├─ 读取分析
   └─ 转换为文字描述
6. 组合：文本 + 图片描述
7. 返回纯 Markdown
```

### 图片 → 文字描述

图片不会被保存，而是转换为详细的文字描述：

```markdown
---
**[图片: 流程图]** 图片展示了用户注册流程：
1. 用户填写邮箱和密码
2. 系统发送验证邮件
3. 用户点击验证链接
4. 注册完成，跳转到首页
箭头显示：每一步之间有"成功/失败"两个分支
---
```

---

## 安全阈值

```typescript
const SAFETY_LIMITS = {
  // === 单文件限制 ===
  image: {
    maxSize: 300 * 1024,        // 300KB（压缩后）
    maxWidth: 800,               // 800px
    maxHeight: 4000,             // 4000px（长图限制）
  },
  pdf: {
    maxPages: 20,                // 最多处理 20 页
  },
  text: {
    maxChars: 50000,             // 单文本最大 5 万字符
    chunkSize: 10000,            // 分块大小 1 万字符
  },
  url: {
    maxSnapshotChars: 40000,     // 快照最大 4 万字符
    maxScreenshotSegments: 5,    // 最多 5 个滚动截图
  },
  // === 批次限制 ===
  batch: {
    maxFiles: 5,                 // 单批次最多 5 个文件
    maxTotalSize: 1 * 1024 * 1024, // 单批次最大 1MB
    maxImages: 3,                // 单批次最多 3 张图
  },
};
```

---

## 组件结构

```
knowledge-etl/
├── agents/
│   ├── extractor.md           # 内容提取 (本地文件)
│   ├── crawler-coordinator.md # 爬虫协调
│   ├── crawler-summarizer.md  # 爬取汇总
│   └── output-transformer.md  # 格式转换
├── commands/
│   └── extract.md             # 入口命令 (MCP 可用)
├── skills/
│   ├── extract/SKILL.md       # 提取技能
│   ├── content-safeguard/     # 内容安全守护
│   ├── relevance-scorer/      # 相关性评分
│   └── plugin-troubleshooting/# 问题排查技能
├── config/
│   └── limits.yaml            # 集中配置
└── scripts/
    ├── compress-image.*       # 图片压缩
    └── crawler-queue.sh       # 爬虫队列管理
```

## Agent 职责边界

| 组件 | 上下文 | MCP | 职责 |
|------|--------|-----|------|
| `extract.md` | 主 | ✅ | URL 捕获，爬虫循环控制 |
| `extractor` | 隔离 | ❌ | 处理本地快照/图片/PDF |
| `crawler-summarizer` | 隔离 | ❌ | 生成 INDEX.md/REPORT.md |
| `output-transformer` | 隔离 | ❌ | 转换为 skill/prompt/rag |

---

## 错误处理

| 问题 | 处理方式 |
|------|----------|
| 需要登录 | 提示用户登录后重试 |
| 反爬保护 | 自动切换到截图模式 |
| 图片过大 | 压缩后再分析 |
| prompt too large | 分段处理 |

| 错误码 | 含义 | 处理方式 |
|--------|------|----------|
| E001 | 输入源不存在 | 提示用户检查路径 |
| E002 | 无访问权限 | 提示用户检查权限 |
| E003 | 网络超时 | 询问是否重试 |
| E004 | 需要登录 | 等待用户登录 |
| E005 | 反爬拦截 | 切换截图模式 |
| E006 | 文件过大 | 压缩或摘要模式 |
| E007 | 不支持的格式 | 跳过并记录 |
| E008 | 处理超限 | 分批或摘要模式 |

---

## 性能基准

| 场景 | 预期时间 | 输出大小 |
|------|----------|----------|
| 单个网页（无图） | 5-10s | 2-10KB |
| 单个网页（5张图） | 15-30s | 5-15KB |
| 单张大图片 | 5-10s | 0.5-1KB |
| 10 页 PDF | 20-40s | 10-20KB |
| 20 个文件目录 | 2-5min | 20-50KB |

---

## 版本历史

- **2.2.0**: 完善系统架构设计，五层流水线架构
- **2.1.0**: 简化架构，图片转文字描述，不保存文件
- **2.0.0**: 升级为 Knowledge ETL，统一提取入口
- **1.0.0**: 初始 web-knowledge-extractor

## License

MIT
