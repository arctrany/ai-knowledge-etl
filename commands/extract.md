---
description: Extract content from URL, images, PDF, directory, or git repo - with optional crawling and output transformation
allowed-tools:
  - mcp__plugin_knowledge-etl_playwright__browser_navigate
  - mcp__plugin_knowledge-etl_playwright__browser_wait_for
  - mcp__plugin_knowledge-etl_playwright__browser_snapshot
  # â›” browser_take_screenshot REMOVED - embeds image into context causing overflow!
  - mcp__plugin_knowledge-etl_playwright__browser_close
  - mcp__plugin_knowledge-etl_playwright__browser_click
  - mcp__plugin_knowledge-etl_playwright__browser_type
  - mcp__plugin_knowledge-etl_playwright__browser_scroll
  - mcp__plugin_knowledge-etl_playwright__browser_evaluate
  - mcp__plugin_knowledge-etl_playwright__browser_press_key
arguments:
  - name: source
    description: URL, image path, glob pattern, PDF path, directory, or git URL
    required: true
  - name: --with-depth
    description: "Enable crawling with specified depth (1-3). Example: --with-depth=2"
    required: false
  - name: --topic
    description: "Topic regex for relevance filtering. Example: --topic=\"API|æ¥å£|REST\""
    required: false
  - name: --max-pages
    description: "Maximum pages to crawl (default: 20). Example: --max-pages=50"
    required: false
  - name: --pipe
    description: "Transform output to format: skill, plugin, prompt, rag, docs, json"
    required: false
  - name: --output-dir
    description: "Output directory (default: .knowledge-etl). Example: --output-dir=./my-output"
    required: false
  - name: --engine
    description: "Extraction engine: auto (default), playwright, jina, trafilatura. See config/security.yaml for routing rules"
    required: false
  - name: --with-images
    description: "Extract and analyze images (default: false). Increases processing time"
    required: false
  - name: --compact-cph
    description: "Compact Chain of Thought - reduce verbose progress output, only show essential status"
    required: false
---

# Knowledge ETL Extract Command

Unified extraction that converts **any content source to pure text Markdown**. Supports crawling with depth traversal and output transformation to various formats.

---

## â›” CRITICAL: MAIN CONTEXT SAFETY RULES

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš¨ PROMPT TOO LONG = PLUGIN FAILURE - 100% PREVENTION REQUIRED ğŸš¨        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘  â›”â›”â›” ABSOLUTELY FORBIDDEN IN MAIN CONTEXT â›”â›”â›”                        â•‘
â•‘                                                                           â•‘
â•‘  âŒ browser_take_screenshot - EMBEDS IMAGE INTO CONTEXT!                 â•‘
â•‘     Even ONE screenshot can cause overflow. NEVER use it.                â•‘
â•‘     Tool has been REMOVED from allowed-tools list.                       â•‘
â•‘                                                                           â•‘
â•‘  âŒ Read(snapshot.md) - could be 500KB+                                   â•‘
â•‘  âŒ Read(screenshot.png) - could be 2MB+                                  â•‘
â•‘  âŒ Read any captured content file                                        â•‘
â•‘  âŒ Read any user-provided large file                                     â•‘
â•‘                                                                           â•‘
â•‘  ONLY allowed in MAIN context:                                           â•‘
â•‘  âœ… browser_snapshot - TEXT only, saves to file, no context impact       â•‘
â•‘  âœ… browser_evaluate - extract data as text/JSON                         â•‘
â•‘  âœ… browser_navigate, wait_for, click, close - navigation only           â•‘
â•‘  âœ… Bash: stat, head -n 10, wc -l (size/preview only)                    â•‘
â•‘  âœ… Bash: curl for image download (no context impact)                    â•‘
â•‘  âœ… Task(subagent) - delegate ALL content reading                        â•‘
â•‘                                                                           â•‘
â•‘  ALL content processing â†’ Task(extractor) in isolated context            â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## STEP 0: Engine Selection & Security Check (REQUIRED)

**Reference**: `config/security.yaml` for URL routing rules.

### Engine Selection Logic

```bash
# Determine extraction engine based on URL and --engine flag

select_engine() {
  local url="$1"
  local requested_engine="${2:-auto}"

  # Extract domain from URL
  domain=$(echo "$url" | sed -E 's|https?://([^/]+).*|\1|')

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # SECURITY CHECK: Force local for internal/sensitive URLs
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  # Internal domains (from security.yaml)
  INTERNAL_PATTERNS=(
    "*.alibaba-inc.com"
    "alidocs.dingtalk.com"
    "*.yuque.com"
  )

  # Private networks
  PRIVATE_PATTERNS=(
    "localhost" "127.0.0.1" "::1"
    "10.*" "192.168.*" "172.16.*" "172.17.*" "172.18.*" "172.19.*"
    "172.20.*" "172.21.*" "172.22.*" "172.23.*" "172.24.*" "172.25.*"
    "172.26.*" "172.27.*" "172.28.*" "172.29.*" "172.30.*" "172.31.*"
  )

  # Sensitive URL patterns
  SENSITIVE_PATTERNS=(
    "*login*" "*signin*" "*auth*" "*oauth*" "*sso*"
    "*admin*" "*dashboard*" "*internal*" "*intranet*"
  )

  # Check if URL matches any force_local pattern
  for pattern in "${INTERNAL_PATTERNS[@]}" "${PRIVATE_PATTERNS[@]}"; do
    if [[ "$domain" == $pattern ]]; then
      echo "playwright"  # Force local
      return
    fi
  done

  for pattern in "${SENSITIVE_PATTERNS[@]}"; do
    if [[ "$url" == $pattern ]]; then
      echo "playwright"  # Force local
      return
    fi
  done

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # USER REQUESTED ENGINE
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  case "$requested_engine" in
    playwright|jina|trafilatura)
      echo "$requested_engine"
      ;;
    auto)
      # Default to playwright (safest)
      # Can try jina for public URLs if configured
      echo "playwright"
      ;;
    *)
      echo "playwright"
      ;;
  esac
}
```

### Engine Capabilities

| Engine | é€Ÿåº¦ | ç™»å½•æ”¯æŒ | å›¾ç‰‡æå– | éšç§å®‰å…¨ | é€‚ç”¨åœºæ™¯ |
|--------|------|----------|----------|----------|----------|
| **playwright** | æ…¢ | âœ… | âœ… | âœ… æœ¬åœ° | å†…éƒ¨ç³»ç»Ÿã€éœ€ç™»å½• |
| **jina** | å¿« | âŒ | âš ï¸ URL | âŒ ç¬¬ä¸‰æ–¹ | å…¬å¼€æ–‡æ¡£ |
| **trafilatura** | ä¸­ | âŒ | âŒ | âœ… æœ¬åœ° | å…¬å¼€æ–‡ç«  |

### Security Output

```
â”Œâ”€ SECURITY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ URL:    alidocs.dingtalk.com/...               â”‚
â”‚ Domain: alidocs.dingtalk.com                   â”‚
â”‚ Match:  internal_domains                       â”‚
â”‚ Engine: playwright (forced local)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## STEP 1: Task Analysis & Plan Output (REQUIRED)

**Before executing, analyze the task complexity and output a plan:**

```
â”Œâ”€ PLAN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Engine:      {playwright|jina|trafilatura}     â”‚
â”‚ 1. Extract   â†’ {what}                          â”‚
â”‚ 2. Process   â†’ {what} â•‘ {parallel}             â”‚
â”‚ 3. Transform â†’ {pipe} (if specified)           â”‚
â”‚ 4. Validate  â†’ {validator}                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Task Complexity Detection

| Condition | Complexity | Plan Steps |
|-----------|------------|------------|
| Single URL, no pipe | Simple | Extract only |
| Single URL + pipe | Medium | Extract â†’ Transform â†’ Validate |
| --with-depth | Complex | Crawl â†’ Summarize â†’ Transform â†’ Validate |
| Directory (>10 files) | Complex | Scan â†’ Batch Extract â†’ Merge |
| Large image (>1MB) | Medium | Compress â†’ Extract |
| Git repo | Complex | Clone â†’ Scan â†’ Extract |

### Plan Examples

**Simple (single page - internal):**
```
â”Œâ”€ PLAN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Engine:      playwright (forced: internal)     â”‚
â”‚ 1. Extract   â†’ capture page snapshot           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Simple (single page - public with jina):**
```
â”Œâ”€ PLAN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Engine:      jina (public URL)                 â”‚
â”‚ 1. Extract   â†’ curl r.jina.ai/{url}            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Medium (single page + skill):**
```
â”Œâ”€ PLAN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Engine:      playwright                        â”‚
â”‚ 1. Extract   â†’ capture page                    â”‚
â”‚ 2. Transform â†’ skill (built-in template)       â”‚
â”‚ 3. Validate  â†’ self-check output size          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Complex (crawl + skill):**
```
â”Œâ”€ PLAN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Engine:      playwright                        â”‚
â”‚ 1. Crawl     â†’ depth:2 max:20 topic-filter     â”‚
â”‚ 2. Summarize â†’ INDEX.md + REPORT.md            â”‚
â”‚ 3. Transform â†’ skill (built-in template)       â”‚
â”‚ 4. Validate  â†’ self-check output size          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Complex (directory with parallel):**
```
â”Œâ”€ PLAN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Scan      â†’ count files, detect types       â”‚
â”‚ 2. Extract   â†’ batch(5) â•‘ compress â•‘ describe  â”‚
â”‚ 3. Merge     â†’ combine all to INDEX.md         â”‚
â”‚ 4. Transform â†’ rag                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Progress Output During Execution (USE TodoWrite)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš¨ USE TodoWrite FOR PROGRESS - NEVER TEXT OUTPUT ğŸš¨                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘  âŒ WRONG: print "[Extract] â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 20% | page loaded"                â•‘
â•‘  âŒ WRONG: Output text after each step (causes context overflow!)        â•‘
â•‘                                                                           â•‘
â•‘  âœ… RIGHT: Use TodoWrite to update task status                           â•‘
â•‘  âœ… RIGHT: TodoWrite renders in UI statusline (persistent, no context)   â•‘
â•‘                                                                           â•‘
â•‘  WHY: Text output accumulates â†’ "Prompt is too long" error               â•‘
â•‘       TodoWrite UI updates â†’ Zero context growth                         â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**TodoWrite Usage Pattern:**

```javascript
// STEP 1: Initialize at start
TodoWrite({
  todos: [
    { content: "Extract page content", status: "in_progress", activeForm: "Navigating to URL..." },
    { content: "Process images", status: "pending", activeForm: "Processing images" },
    { content: "Transform output", status: "pending", activeForm: "Transforming output" },
    { content: "Validate results", status: "pending", activeForm: "Validating results" }
  ]
})

// STEP 2: Update activeForm during work
// After navigate:
TodoWrite({ todos: [
  { content: "Extract page content", status: "in_progress", activeForm: "Page loaded, capturing snapshot..." },
  ...
]})

// After snapshot:
TodoWrite({ todos: [
  { content: "Extract page content", status: "in_progress", activeForm: "Snapshot captured (12K chars)" },
  ...
]})

// STEP 3: Mark complete, start next
TodoWrite({ todos: [
  { content: "Extract page content", status: "completed", activeForm: "Extracted page content" },
  { content: "Process images", status: "in_progress", activeForm: "Downloading images (3/5)..." },
  ...
]})
```

**activeForm Examples:**
- `"Navigating to URL..."` â†’ `"Page loaded, waiting..."` â†’ `"Capturing snapshot..."`
- `"Downloading images (2/5)..."` â†’ `"Compressing (1.2MBâ†’280KB)..."`
- `"â¸ LOGIN REQUIRED - complete in browser"`
- `"âš  Anti-scrape detected, using screenshot"`
- `"âœ“ Done: extracted.md (8K chars)"`

**Final Summary (text output only at completion):**
```markdown
### âœ“ Extraction Complete
- Output: `.knowledge-etl/extracted.md` (8,234 chars)
- Images: 5 processed, 2 skipped
- Time: 15.2s
```

---

## Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    PLUGGABLE PIPELINE ARCHITECTURE                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘  [Input Sources]        [Core]           [Output Pipes]                   â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â•‘
â•‘  â€¢ URL (single)   â”€â”                 â”Œâ”€â–¶ â€¢ --pipe=skill                   â•‘
â•‘  â€¢ URL (crawl)    â”€â”¤                 â”œâ”€â–¶ â€¢ --pipe=plugin                  â•‘
â•‘  â€¢ Local file     â”€â”¼â”€â”€â–¶ Extractor â”€â”€â”€â”¼â”€â–¶ â€¢ --pipe=prompt                  â•‘
â•‘  â€¢ Directory      â”€â”¤    Agent        â”œâ”€â–¶ â€¢ --pipe=rag                     â•‘
â•‘  â€¢ Glob pattern   â”€â”¤    (isolated)   â”œâ”€â–¶ â€¢ --pipe=docs                    â•‘
â•‘  â€¢ Git repo       â”€â”˜                 â””â”€â–¶ â€¢ --pipe=json                    â•‘
â•‘                                                                           â•‘
â•‘  IRON RULE: Every operation runs in isolated agent context               â•‘
â•‘             to PREVENT "Prompt Too Long" errors.                          â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## Input

- **Source**: `$ARGUMENTS.source`
  - URL (http/https) - single page or with --with-depth for crawling
  - Image path (png, jpg, etc.)
  - Glob pattern (*.png, docs/*.md)
  - PDF path
  - Directory path
  - Git URL (git@..., https://github.com/...)

## Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--with-depth=N` | Enable crawling with depth N (1-3) | disabled |
| `--topic=REGEX` | Topic filter regex for relevance scoring | none (all) |
| `--max-pages=N` | Maximum pages to crawl | 20 |
| `--pipe=FORMAT` | Transform output: skill, plugin, prompt, rag, docs, json | none |
| `--output-dir=PATH` | Output directory | .knowledge-etl |

## Output

**Pure Markdown text** with:
- YAML frontmatter (source, title, stats)
- Text content
- Image descriptions (text, not files)

**With --pipe**: Additional formatted output in `output/{format}/`

## Execution

### Engine Dispatch (STEP 2)

Based on STEP 0 security check, dispatch to appropriate engine:

```
â”Œâ”€ ENGINE DISPATCH â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                   â”‚
â”‚  engine = "playwright" (default/internal)                         â”‚
â”‚  â”œâ”€â”€ Use Playwright MCP for full browser automation             â”‚
â”‚  â”œâ”€â”€ Supports: login, JS rendering, images, cookies             â”‚
â”‚  â””â”€â”€ See: "For URLs (playwright)" section below                  â”‚
â”‚                                                                   â”‚
â”‚  engine = "jina" (public URLs only)                               â”‚
â”‚  â”œâ”€â”€ Fast: Single curl call to r.jina.ai                         â”‚
â”‚  â”œâ”€â”€ Clean: Returns pure Markdown directly                       â”‚
â”‚  â”œâ”€â”€ Script: scripts/extract-jina.sh                             â”‚
â”‚  â””â”€â”€ Usage:                                                       â”‚
â”‚      Bash: "${CLAUDE_PLUGIN_ROOT}/scripts/extract-jina.sh"       â”‚
â”‚            "{URL}" "{OUTPUT_DIR}"                                 â”‚
â”‚                                                                   â”‚
â”‚  engine = "trafilatura" (local, no JS)                            â”‚
â”‚  â”œâ”€â”€ Local: No external API calls                                â”‚
â”‚  â”œâ”€â”€ Script: scripts/extract-trafilatura.sh                      â”‚
â”‚  â””â”€â”€ Usage:                                                       â”‚
â”‚      Bash: "${CLAUDE_PLUGIN_ROOT}/scripts/extract-trafilatura.sh"â”‚
â”‚            "{URL}" "{OUTPUT_DIR}"                                 â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Jina Engine Example:**
```bash
# For public URLs - fast extraction
Bash("${CLAUDE_PLUGIN_ROOT}/scripts/extract-jina.sh" \
     "https://docs.python.org/3/library/json.html" \
     ".knowledge-etl/python-json")

# Output: .knowledge-etl/python-json/docs_python_org_3_library_json_html.md
```

**Security Block Example:**
```bash
# Internal URL â†’ Script will exit with error code 2
Bash("${CLAUDE_PLUGIN_ROOT}/scripts/extract-jina.sh" \
     "https://alidocs.dingtalk.com/..." \
     ".knowledge-etl/")

# Output: [error] Security: alidocs.dingtalk.com is an internal domain.
# Action: Fall back to playwright engine
```

---

### For URLs (playwright engine)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âš ï¸ CRITICAL: MCP TOOLS NOT AVAILABLE IN SUBAGENTS                        â•‘
â•‘                                                                            â•‘
â•‘  Playwright MCP tools only work in MAIN context, not in Task agents.      â•‘
â•‘  Solution: Main context captures URL, then delegates LOCAL files.         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Step-by-step execution (3-phase):**

```
PHASE 1: MAIN CONTEXT - Capture URL content + Extract Images
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Execute Playwright in main context (MCP tools available here):

Step 1.1: Navigate and capture page (TEXT ONLY - NO SCREENSHOT!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. mcp__playwright__browser_navigate(url: "{URL}")
2. mcp__playwright__browser_wait_for(time: 3)
3. mcp__playwright__browser_press_key(key: "End")  # Trigger lazy-load
4. mcp__playwright__browser_wait_for(time: 2)
5. mcp__playwright__browser_snapshot(filename: "snapshot.md")
# â›” NO browser_take_screenshot - it embeds image into context!

Step 1.2: Check for login (NEVER read full file!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Use Bash to check first 10 lines ONLY:
  head -n 10 .playwright-mcp/snapshot.md | grep -iE "ç™»å½•|login|password|SSO|sign.?in"
- If login detected:
  - AskUserQuestion: "è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•"
  - Re-capture after user confirms

Step 1.3: Extract image URLs from page (NEW)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use browser_evaluate to get image URLs with filtering:

mcp__playwright__browser_evaluate({
  function: "() => {
    const imgs = Array.from(document.querySelectorAll('img'));
    return imgs
      .filter(img => {
        // Filter out decorative images
        const w = img.naturalWidth || img.width;
        const h = img.naturalHeight || img.height;
        if (w < 100 || h < 100) return false;

        const src = (img.src || '').toLowerCase();
        const alt = (img.alt || '').toLowerCase();

        // Skip icons, logos, avatars
        const skipPatterns = ['icon', 'logo', 'avatar', 'emoji', 'button', 'arrow'];
        if (skipPatterns.some(p => src.includes(p) || alt.includes(p))) return false;

        return true;
      })
      .slice(0, 5)  // Max 5 images
      .map((img, i) => ({
        index: i,
        src: img.src,
        alt: img.alt || '',
        width: img.naturalWidth || img.width,
        height: img.naturalHeight || img.height
      }));
  }"
})

â†’ Write result to: .playwright-mcp/images.json (using Bash echo)

Step 1.4: Download images (CONTEXT-SAFE METHOD)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš¨ NEVER USE browser_take_screenshot IN A LOOP FOR IMAGE DOWNLOAD ğŸš¨    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘  Each screenshot embeds image data into conversation context!            â•‘
â•‘  Multiple screenshots = RAPID context explosion = "Prompt is too long"   â•‘
â•‘                                                                           â•‘
â•‘  âŒ DEPRECATED: Click-to-preview screenshot loop                         â•‘
â•‘  âœ… CORRECT: browser_evaluate + curl download (zero context impact)      â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

Download images using curl (ONLY correct method)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Get cookies for authenticated image download
mcp__playwright__browser_evaluate({
  function: "() => document.cookie"
})
â†’ Store as $COOKIES

# Download images in parallel (Bash)
for each image in images.json:
  curl -s -L \
    -H "Cookie: $COOKIES" \
    -H "Referer: {URL}" \
    -H "User-Agent: Mozilla/5.0..." \
    --max-time 10 \
    -o ".playwright-mcp/img_{index}.jpg" \
    "{image.src}" &
wait  # Wait for all downloads

# Verify downloads - skip invalid files (don't use screenshot fallback!)
for img in .playwright-mcp/img_*.jpg:
  # Check if file is actually an image or an error page
  file_type=$(file -b "$img" | head -c 10)
  if [[ "$file_type" != "JPEG"* && "$file_type" != "PNG"* && "$file_type" != "WebP"* ]]; then
    echo "[Extract] âš  download failed for $img, skipping (auth-protected)"
    rm -f "$img"  # Remove invalid file
    # NOTE: Do NOT use screenshot fallback - causes context overflow!
  fi

# Compress large images
for img in .playwright-mcp/img_*.jpg:
  SIZE=$(stat -f%z "$img" 2>/dev/null || stat -c%s "$img" 2>/dev/null || echo "0")
  if [ "$SIZE" -gt 300000 ]; then
    "${CLAUDE_PLUGIN_ROOT}/scripts/compress-image.sh" "$img" "$img" 800
  fi

print "[Extract] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80% | images captured"

Step 1.5: Close browser
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mcp__playwright__browser_close()

PHASE 2: SUBAGENT - Process local files (isolated context)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Delegate to extractor agent for content processing:

Task(
  subagent_type: "knowledge-etl:extractor",
  prompt: """
    Process captured content from: {URL}

    Local files available:
    - .playwright-mcp/snapshot.md (text content)
    - .playwright-mcp/screenshot.png (visual fallback)
    - .playwright-mcp/images.json (image metadata)
    - .playwright-mcp/img_*.jpg (downloaded page images, pre-compressed)

    âš ï¸ EXTRACTION PRIORITY:
    Snapshot FIRST (text) â†’ Screenshot FALLBACK ONLY (when blocked)

    Steps:
    1. Read snapshot.md FIRST (chunk if >500 lines)
    2. Read images.json for image metadata
    3. For each img_*.jpg (if exists):
       a. Check size (<300KB required)
       b. Read ONE image at a time
       c. Describe image (type + content)
       d. Release from context before next image
    4. Combine: text content + image descriptions
    5. Write output to: {output_dir}/extracted.md

    Output format:
    ---
    source: {URL}
    title: {extracted_title}
    stats:
      chars: {count}
      images: {processed_count}
    ---

    {text_content}

    ---
    **[Image 1: {alt or description}]**
    Type: {flowchart|architecture|screenshot|chart}
    {detailed_visual_description}
    ---

    Follow safety limits strictly.
  """
)
```

**Why 2-phase:**
| Phase | Context | MCP Tools | Purpose |
|-------|---------|-----------|---------|
| 1 | Main | âœ… Available | Network capture |
| 2 | Subagent | âŒ Not available | Content processing (isolated) |

**Why this works:**

| Aspect | Old Approach (broken) | New Approach (correct) |
|--------|----------------------|------------------------|
| Playwright runs in | Main context | Extractor agent (isolated) |
| Screenshot data goes to | Main context â†’ OVERFLOW | Agent context â†’ safe |
| Context isolation | Partial (only processing) | Complete (fetch + process) |

### For Local Files (Images, PDFs, Directories)

**Delegate directly to the extractor agent** for isolated context:

```
Task(
  subagent_type: "knowledge-etl:extractor",
  prompt: """
    Extract content from: $ARGUMENTS.source

    Requirements:
    - Extract all text content
    - Convert images to text descriptions
    - Return pure Markdown, no file references
    - Follow safety limits strictly
    - Use fallback strategies when needed
  """
)
```

## Safety Limits (Enforced by Agent)

| Resource | Limit |
|----------|-------|
| Image size | 300 KB max (compress if larger) |
| Image width | 800 px max |
| Images per session | 5 max |
| Text per file | 20,000 chars (truncate beyond) |
| Snapshot | 30,000 chars (truncate beyond) |
| Total output | 50,000 chars |
| PDF pages | 15 max |
| Batch size | 5 files |

## Processing Strategies

| Input Type | Strategy |
|------------|----------|
| Single URL | Snapshot â†’ Screenshot fallback |
| Single Image | Size check â†’ Compress â†’ Describe |
| Single PDF | Read â†’ Summarize if >15 pages |
| Multiple Images | One-by-one â†’ Describe each â†’ Combine |
| Directory | Scan â†’ Batch(5) â†’ Summarize each |

## Obstacle Handling

| Obstacle | Resolution |
|----------|------------|
| Login required | Report to user, wait for login |
| Anti-scrape | Screenshot + visual analysis |
| Prompt too large | Segment processing |
| Image unreadable | Mark as "[cannot read image]" |

## Output Format

```markdown
---
source: [URL or path]
title: [Extracted title]
type: [url|image|pdf|directory]
extracted_at: [ISO timestamp]
stats:
  chars: [total chars]
  images: [images processed]
  files: [files processed]
---

# [Title]

[Text content...]

---
**[Image 1: Description]**
[Detailed image description in text]
---

[More content...]
```

## Examples

**Web page:**
```
/knowledge-etl:extract https://docs.example.com/guide

â†’ Extracts text via snapshot
â†’ Describes images in text
â†’ Returns pure Markdown
```

**Large screenshot:**
```
/knowledge-etl:extract ./screenshot-4k.png

â†’ Compresses to 800px width
â†’ Extracts visible text (OCR)
â†’ Describes visual elements
â†’ Returns text description
```

**Directory:**
```
/knowledge-etl:extract ./docs/

â†’ Scans directory (excludes node_modules, .git)
â†’ Processes in batches of 5
â†’ Generates summary per file
â†’ Returns combined index
```

**Glob pattern:**
```
/knowledge-etl:extract "./images/*.png"

â†’ Expands glob to file list
â†’ Processes one by one
â†’ Describes each image
â†’ Returns combined text
```

---

## Crawl Mode (--with-depth)

When `--with-depth` is specified, execute multi-page extraction with URL capture loop:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    CRAWL MODE EXECUTION                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘  âš ï¸ MCP tools only work in MAIN context (this command).                   â•‘
â•‘  URL capture loop runs HERE, processing delegated to subagents.          â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

STEP 1: Initialize crawl state
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mkdir -p {output_dir}/pages {output_dir}/links
Write config.json with: depth, topic, max_pages, entry_url
Write queue.json with: [{url: entry_url, depth: 0, priority: 10}]
Write visited.json with: {urls: {}, count: 0}

STEP 2: URL Capture Loop (MAIN CONTEXT - MCP available)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
page_id = 0
while queue not empty AND page_id < max_pages:

  # 2a. Read queue, get highest priority URL
  queue = Read(queue.json)
  url = pop_highest_priority(queue)

  # Skip if visited
  if url in visited: continue

  page_id += 1

  # 2b. Capture URL using Playwright (TEXT SNAPSHOT ONLY!)
  mcp__playwright__browser_navigate(url: url)
  mcp__playwright__browser_wait_for(time: 3)
  mcp__playwright__browser_snapshot(filename: "page_{page_id}.md")
  # â›” NO browser_take_screenshot - causes context overflow!

  # Check for login (NEVER Read full file - use Bash head only!)
  # Bash: head -n 10 .playwright-mcp/page_{page_id}.md | grep -iE "ç™»å½•|login|password|SSO"
  if login_detected:
    AskUserQuestion("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•")
    # Re-capture after login

  # 2c. Delegate processing to extractor (isolated context)
  Task(
    subagent_type: "knowledge-etl:extractor",
    prompt: "Process .playwright-mcp/page_{page_id}.md ..."
  )
  # Extractor writes: pages/{page_id}_*.md, links/{page_id}.json

  # 2d. Read extracted links, add to queue
  links = Read("{output_dir}/links/{page_id}.json")
  for link in links:
    if link.depth <= max_depth AND link.relevance >= 5:
      add_to_queue(link, priority=link.relevance + parent_bonus)

  # 2e. Mark URL as visited
  add_to_visited(url, page_id)

  # 2f. Report progress
  print("[Crawl] {page_id}/{max_pages} | {url} | relevance: {score}")

mcp__playwright__browser_close()

STEP 3: Summarize (delegate to summarizer)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Task(
  subagent_type: "knowledge-etl:crawler-summarizer",
  prompt: "Generate INDEX.md and REPORT.md for {output_dir}..."
)

STEP 4: Transform (if --pipe specified)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if pipe:
  Task(
    subagent_type: "knowledge-etl:output-transformer",
    prompt: "Transform to {pipe} format..."
  )

STEP 5: Return summary
```

**Crawl Examples:**

```bash
# Basic crawl with depth 2
/knowledge-etl:extract https://docs.example.com --with-depth=2

# Crawl with topic filter (regex)
/knowledge-etl:extract https://api.example.com --with-depth=2 --topic="API|REST|endpoint|è®¤è¯"

# Crawl and convert to skill
/knowledge-etl:extract https://docs.example.com/api --with-depth=2 --topic="API" --pipe=skill

# Crawl and generate RAG chunks
/knowledge-etl:extract https://docs.example.com --with-depth=3 --max-pages=50 --pipe=rag
```

---

## Pipeline Mode (--pipe)

When `--pipe` is specified, transform extracted content to target format:

```
Supported formats:
  --pipe=skill    â†’ Claude Code Skill (skill.yaml + SKILL.md)
  --pipe=plugin   â†’ Claude Code Plugin structure
  --pipe=prompt   â†’ System prompt for LLMs
  --pipe=rag      â†’ RAG-friendly chunks for vector DB
  --pipe=docs     â†’ Documentation structure
  --pipe=json     â†’ Structured JSON knowledge base
```

**Pipeline is executed AFTER extraction completes:**

```
1. Extraction phase (extractor or crawler)
   â†’ Raw content saved to {output_dir}/pages/

2. Summarization phase (if crawl mode)
   â†’ INDEX.md and REPORT.md generated

3. Transformation phase (if --pipe specified)
   â†’ Task(
       subagent_type: "knowledge-etl:output-transformer",
       prompt: """
         Transform extracted content to: {pipe} format

         Source directory: {output_dir}
         Topic: {topic}

         Read REPORT.md (or single page content) and transform to
         {pipe} format. Follow the template for that format.

         Output to: {output_dir}/output/{pipe}/
       """
     )

4. Return summary and output location
```

**Pipeline Examples:**

```bash
# Single page to skill
/knowledge-etl:extract https://docs.example.com/quick-start --pipe=skill

# Directory to RAG chunks
/knowledge-etl:extract ./docs --pipe=rag

# Git repo to system prompt
/knowledge-etl:extract https://github.com/user/library --pipe=prompt

# Crawl to plugin
/knowledge-etl:extract https://api.example.com --with-depth=2 --topic="API" --pipe=plugin
```

---

## Git Repository Support

When source is a git URL, extract documentation and code signatures:

```
Detection patterns:
  â€¢ git@github.com:user/repo.git
  â€¢ https://github.com/user/repo.git
  â€¢ https://github.com/user/repo

Extraction:
  1. Clone repo (shallow, to temp dir)
  2. Process README and docs/*.md
  3. Extract API signatures from code
  4. Generate codebase overview

Example:
/knowledge-etl:extract https://github.com/user/awesome-lib --pipe=skill
```

---

## Output Directory Structure

```
{output_dir}/
â”œâ”€â”€ config.json           # Extraction configuration
â”œâ”€â”€ pages/                # Extracted page content
â”‚   â”œâ”€â”€ 001_*.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ links/                # Discovered links (crawl mode)
â”‚   â””â”€â”€ *.json
â”œâ”€â”€ INDEX.md              # Page index (crawl mode)
â”œâ”€â”€ REPORT.md             # Topic report (crawl mode)
â””â”€â”€ output/               # Transformed output (--pipe)
    â””â”€â”€ {format}/
        â””â”€â”€ ...
```

---

## Complete Execution Flow

### Step 1: Analyze & Plan

```
1. Parse $ARGUMENTS.source and options
2. Detect task complexity (simple/medium/complex)
3. Generate plan steps
4. OUTPUT PLAN to user (REQUIRED)
```

### Step 2: Execute Plan Steps Sequentially

```
FOR EACH step in plan:
  1. Output progress line: [N/M Step] â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ X% | status...
  2. Execute step
  3. Report key result (file, size, status)
  4. Continue to next step

IF any step fails:
  â†’ Output warning: [N/M Step] â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ X% | âš  reason
  â†’ Apply fallback if available
  â†’ Continue or abort based on severity
```

### Step 3: Final Summary

```
â”â”â” DONE (Xs) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ âœ“ Xs
Step 2 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ âœ“ Xs
...

â†’ output files list
```

---

## Execution Modes

```
Parse $ARGUMENTS.source and options

IF source is URL AND --with-depth specified:
  â†’ Crawl mode: Use crawler-coordinator agent

ELSE IF source is git URL:
  â†’ Git mode: Clone and extract docs

ELSE:
  â†’ Single extraction: Use extractor agent

THEN IF --pipe specified:
  â†’ Transform: Use output-transformer agent

FINALLY:
  â†’ Return summary and output location
```
