---
name: Knowledge ETL Extract
description: |-
  Unified extraction - converts any source to pure text Markdown.
  Supports crawling, relevance scoring, and output transformation.

  Use this skill when:
  - Extracting content from web pages (with anti-scrape handling)
  - Crawling websites with depth (--with-depth)
  - Analyzing local images (especially large ones)
  - Processing PDFs, directories, or git repositories
  - Handling "prompt too large" errors
  - Transforming content to skill, plugin, prompt, RAG formats

  Key capability: All operations run in isolated agent context to PREVENT overflow.
  Self-contained with built-in templates. No external plugin dependencies.

  Triggers: "extract from url", "crawl website", "analyze image", "prompt too large",
  "extract content", "create skill from", "generate rag", "extract with depth"
version: 0.1.3
allowed-tools: Read, Write, Bash, Glob, Grep, AskUserQuestion
---

# Knowledge ETL - Extract Skill

Unified extraction with **built-in templates** and **pluggable pipeline**.

## â›” IRON RULE: "Prompt is too long" = PLUGIN FAILURE

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš¨ğŸš¨ğŸš¨ IRON RULE: PREVENT "PROMPT IS TOO LONG" AT ALL COSTS ğŸš¨ğŸš¨ğŸš¨      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘  "Prompt is too long" error = COMPLETE PLUGIN FAILURE                    â•‘
â•‘  This is UNACCEPTABLE and must be prevented with 100% certainty.         â•‘
â•‘                                                                           â•‘
â•‘  ALL content processing MUST run in isolated subagent contexts.          â•‘
â•‘  Main context MUST NOT read any large files (snapshots, images, pages).  â•‘
â•‘                                                                           â•‘
â•‘  Key principles:                                                          â•‘
â•‘  - Main context: Playwright capture ONLY, delegate to subagents          â•‘
â•‘  - Subagents: Check size FIRST, chunk large files, compress images       â•‘
â•‘  - NEVER use Read() without checking file size first                     â•‘
â•‘  - ALWAYS use Read(limit: 500) for files >500 lines                      â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SELF-CONTAINED ARCHITECTURE                            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘  Layer 1: Internal Capabilities                                          â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â•‘
â•‘  content-safeguard â†’ Size limits, compression, truncation                â•‘
â•‘  relevance-scorer  â†’ Regex matching, topic filtering                     â•‘
â•‘                                                                           â•‘
â•‘  Layer 2: Extraction Agents                                              â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â•‘
â•‘  extractor         â†’ LOCAL file processing (snapshot/image/PDF)          â•‘
â•‘  crawler-summarizerâ†’ INDEX.md and REPORT.md generation                   â•‘
â•‘                                                                           â•‘
â•‘  Layer 3: Output Pipelines (Built-in Templates)                          â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â•‘
â•‘  output-transformer â†’ skill, plugin, prompt, rag, docs, json             â•‘
â•‘                                                                           â•‘
â•‘  IRON RULE: Every operation in isolated context - NO overflow            â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## Command Format

```bash
/knowledge-etl:extract <source> [--with-depth=N] [--topic=REGEX] [--max-pages=N] [--pipe=FORMAT]
```

## Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--with-depth=N` | Enable crawling with depth N (1-3) | disabled |
| `--topic=REGEX` | Topic filter regex for relevance | none |
| `--max-pages=N` | Maximum pages to crawl | 20 |
| `--pipe=FORMAT` | Output format: skill, plugin, prompt, rag, docs, json | none |

## Safety Limits

Reference: `config/limits.yaml`

| Resource | Limit |
|----------|-------|
| Image | 300 KB / 800 px / 5 per session |
| Text | 20,000 chars per file |
| Output | 50,000 chars total |
| PDF | 15 pages max |
| Batch | 5 files at a time |

## Relevance Scoring

**Topic as Regex:**
```
--topic="API|æ¥å£|endpoint|REST|è®¤è¯"
```

**Scoring:**
- URL match: +3
- Anchor text match: +5
- Context match: +2
- Score 8-10: Deep exploration (DFS)
- Score 5-7: Breadth scan (BFS)
- Score <5: Skip exploration

## MCP Context Limitation

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  MCP tools (Playwright) only work in MAIN context (commands).            â•‘
â•‘  Subagents cannot access MCP - they process LOCAL files only.            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Execution Model:**
1. Command (main context) captures URL via Playwright â†’ saves locally
2. Extractor agent (isolated) processes local snapshot files
3. Results written to disk, not returned to main context

## Output Pipelines

| Pipe | Output | Template |
|------|--------|----------|
| `--pipe=skill` | SKILL.md + references/ | Built-in skill template |
| `--pipe=plugin` | Full plugin structure | Built-in plugin template |
| `--pipe=prompt` | System prompt for LLMs | Built-in prompt template |
| `--pipe=rag` | Chunked JSON for vector DB | 500-1000 char chunks |
| `--pipe=docs` | Documentation structure | README + reference/ |
| `--pipe=json` | Structured JSON | knowledge.json |

## Examples

```bash
# Single page extraction
/knowledge-etl:extract https://docs.example.com/guide

# Crawl with depth and topic filter (regex)
/knowledge-etl:extract https://api.example.com --with-depth=2 --topic="API|REST"

# Crawl and generate skill
/knowledge-etl:extract https://docs.example.com --with-depth=2 --topic="API" --pipe=skill

# Directory to RAG chunks
/knowledge-etl:extract ./docs --pipe=rag

# Git repo to system prompt
/knowledge-etl:extract https://github.com/user/lib --pipe=prompt
```

## Output Structure

```
.knowledge-etl/
â”œâ”€â”€ config.json          # Configuration
â”œâ”€â”€ pages/               # Extracted content
â”œâ”€â”€ links/               # Discovered links (crawl)
â”œâ”€â”€ INDEX.md             # Page index (crawl)
â”œâ”€â”€ REPORT.md            # Topic report (crawl)
â””â”€â”€ output/{pipe}/       # Transformed output
```

## â›” Image Extraction Rules (CRITICAL)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš¨ NEVER USE browser_take_screenshot FOR IMAGE DOWNLOAD ğŸš¨              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘  Screenshot embeds image data into conversation context!                 â•‘
â•‘  Multiple screenshots = Context explosion = "Prompt is too long"         â•‘
â•‘                                                                           â•‘
â•‘  âŒ WRONG: Click preview â†’ screenshot â†’ loop = CONTEXT OVERFLOW          â•‘
â•‘  âœ… RIGHT: browser_evaluate â†’ extract URLs â†’ curl download               â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Correct Image Download Flow:**
1. `browser_evaluate` - Extract all `img[src]` URLs from page â†’ save to `images.json`
2. `scripts/download-images.sh` - Batch download via curl (no context impact)
3. Subagent processes local image files (isolated context)

**Forbidden Patterns:**
- âŒ `browser_take_screenshot` for downloading images
- âŒ Opening preview modals for extraction
- âŒ Looping screenshots in main context
- âŒ Any operation that embeds images into conversation

## Error Handling

| Error | Resolution |
|-------|------------|
| Login required | AskUserQuestion â†’ wait for user |
| Anti-scrape | Screenshot fallback (single page only, NOT for images) |
| Prompt too large | Apply content-safeguard patterns |
| Image unreadable | Mark "[cannot read image]" |

## Requirements

**MCP:**
- Playwright with persistent profile for URL extraction
